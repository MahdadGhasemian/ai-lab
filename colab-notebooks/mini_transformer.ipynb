{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b56388c5",
      "metadata": {
        "id": "b56388c5"
      },
      "source": [
        "ðŸ”¹ Mini Transformer Block (TensorFlow / Keras)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Large Language Models (LLMs) are a subcategory of machine learning models â€” more specifically, they are deep learning models that use a neural network architecture called the Transformer (introduced in 2017).\n",
        "\n",
        "So the hierarchy looks like this:\n",
        "\n",
        "Artificial Intelligence (AI)\n",
        "\n",
        "  * Machine Learning (ML)\n",
        "\n",
        "    * Deep Learning (DL)\n",
        "\n",
        "      * Neural Networks\n",
        "\n",
        "        * Transformers\n",
        "\n",
        "          * LLMs (like GPT, LLaMA, PaLM, etc.)"
      ],
      "metadata": {
        "id": "qUE1mzryWO5f"
      },
      "id": "qUE1mzryWO5f"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# --- Transformer Block ---\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
        "        super().__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, x):\n",
        "        attn_out = self.att(x, x)\n",
        "        out1 = self.norm1(x + attn_out)\n",
        "        ffn_out = self.ffn(out1)\n",
        "        return self.norm2(out1 + ffn_out)\n",
        "\n",
        "# --- Tiny GPT-like Model ---\n",
        "def build_tiny_gpt(vocab_size, max_len, embed_dim=32, num_heads=2, ff_dim=64):\n",
        "    inputs = layers.Input(shape=(max_len,))\n",
        "    tok_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
        "    pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)(\n",
        "        tf.range(start=0, limit=max_len)\n",
        "    )\n",
        "    x = tok_emb + pos_emb\n",
        "    x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "    outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "# --- Prepare Toy Dataset ---\n",
        "text = \"hello world \" * 100  # repeat phrase\n",
        "vocab = {\"hello\": 0, \"world\": 1}  # tiny vocab\n",
        "reverse_vocab = {v: k for k, v in vocab.items()}\n",
        "\n",
        "# Encode text as numbers\n",
        "encoded = [vocab[word] for word in text.split()]  # [0,1,0,1,0,1,...]\n",
        "\n",
        "# Create sequences (input â†’ next word)\n",
        "seq_len = 2\n",
        "inputs, targets = [], []\n",
        "for i in range(len(encoded) - seq_len):\n",
        "    inputs.append(encoded[i:i+seq_len])\n",
        "    targets.append(encoded[i+1:i+seq_len+1])\n",
        "\n",
        "inputs = tf.constant(inputs)\n",
        "targets = tf.constant(targets)\n",
        "\n",
        "# --- Build and Train ---\n",
        "model = build_tiny_gpt(vocab_size=2, max_len=seq_len)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(inputs, targets, epochs=10, batch_size=8)\n",
        "\n",
        "# --- Test prediction ---\n",
        "import numpy as np\n",
        "\n",
        "def predict_next(seed):\n",
        "    tokens = [vocab[w] for w in seed.split()]\n",
        "    tokens = tf.constant([tokens])  # batch of 1\n",
        "    preds = model(tokens)[0, -1].numpy()  # get last position predictions\n",
        "    next_id = np.argmax(preds)\n",
        "    return reverse_vocab[next_id]\n",
        "\n",
        "print(\"Input: 'hello' â†’ Next word:\", predict_next(\"hello\"))\n",
        "print(\"Input: 'world' â†’ Next word:\", predict_next(\"world\"))\n"
      ],
      "metadata": {
        "id": "TgSbOGX9WsC_"
      },
      "id": "TgSbOGX9WsC_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}